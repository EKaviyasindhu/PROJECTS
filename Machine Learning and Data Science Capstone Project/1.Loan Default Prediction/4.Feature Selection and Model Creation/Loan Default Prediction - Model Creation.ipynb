{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0060d338-1535-473e-95f6-154e3e37bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Imports\n",
    "# ========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report,roc_auc_score,precision_score, recall_score\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2dd94f-8222-4f00-8c13-dcc5424a8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2. Load Preprocessed Data\n",
    "# ========================\n",
    "df = pd.read_csv(\"Prep_Loan_default.csv\")\n",
    "\n",
    "X = df.drop(columns=[\"Default\"])\n",
    "y = df[\"Default\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8668bc6a-0c49-47d5-b8c1-677e0391f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 3. Base Pipeline - Feature Engineering\n",
    "# ========================\n",
    "# Common preprocessing steps for all models\n",
    "base_steps = [\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"selector\", SelectKBest(score_func=f_classif, k=20)),  # tune k if needed\n",
    "    (\"pca\", PCA(n_components=10, random_state=42))          # tune n if needed\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74195ac1-8329-4e5a-941e-81fc094ed2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 4. Define Models & Hyperparameters\n",
    "# ========================\n",
    "models = {\n",
    "    \"Logistic Regression\": (\n",
    "        Pipeline(base_steps + [(\"model\", LogisticRegression(max_iter=1000, random_state=42))]),\n",
    "        {\"model__C\": [0.01, 0.1, 1, 10], \"model__penalty\": [\"l2\"]}\n",
    "    ),\n",
    "    \"Random Forest\": (\n",
    "        Pipeline(base_steps + [(\"model\", RandomForestClassifier(random_state=42))]),\n",
    "        {\"model__n_estimators\": [10, 20], \"model__max_depth\": [3, 5, None]}\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        Pipeline(base_steps + [(\"model\", XGBClassifier(eval_metric=\"logloss\", random_state=42))]),\n",
    "        {\"model__n_estimators\": [10, 20], \"model__max_depth\": [3, 5]}\n",
    "    ),\n",
    "    #\"SVM\": (\n",
    "    #    Pipeline(base_steps + [(\"model\", SVC(probability=True, random_state=42))]),\n",
    "    #    {\"model__C\": [0.1, 1, 10], \"model__kernel\": [\"linear\", \"rbf\"]}\n",
    "    #),\n",
    "    #\"KNN\": (\n",
    "    #    Pipeline(base_steps + [(\"model\", KNeighborsClassifier())]),\n",
    "    #    {\"model__n_neighbors\": [3, 5, 7], \"model__weights\": [\"uniform\", \"distance\"]}\n",
    "    #),\n",
    "    \"Gradient Boosting\": (\n",
    "        Pipeline(base_steps + [(\"model\", GradientBoostingClassifier(random_state=42))]),\n",
    "        {\"model__n_estimators\": [10, 20], \"model__learning_rate\": [0.05, 0.1]}\n",
    "    ),\n",
    "    \"Naive Bayes\": (\n",
    "        Pipeline(base_steps + [(\"model\", GaussianNB())]),\n",
    "        {}  # no hyperparameters\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389504fb-ea16-4e41-a133-06d50d06fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.5656918944365983, 1: 4.30564454936346}\n",
      "\n",
      "Training Logistic Regression ...\n",
      "Logistic Regression -> Accuracy: 0.6166, F1: 0.6855, AUC: 0.6677, Precision: 0.8387, Recall: 0.6166\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.62      0.74     45139\n",
      "           1       0.18      0.62      0.27      5931\n",
      "\n",
      "    accuracy                           0.62     51070\n",
      "   macro avg       0.55      0.62      0.51     51070\n",
      "weighted avg       0.84      0.62      0.69     51070\n",
      "\n",
      "\n",
      "Training Random Forest ...\n",
      "Random Forest -> Accuracy: 0.8839, F1: 0.8294, AUC: 0.6442, Precision: 0.7812, Recall: 0.8839\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     45139\n",
      "           1       0.00      0.00      0.00      5931\n",
      "\n",
      "    accuracy                           0.88     51070\n",
      "   macro avg       0.44      0.50      0.47     51070\n",
      "weighted avg       0.78      0.88      0.83     51070\n",
      "\n",
      "\n",
      "Training XGBoost ...\n",
      "Using scale_pos_weight=7.61\n",
      "XGBoost -> Accuracy: 0.6474, F1: 0.7102, AUC: 0.6871, Precision: 0.8437, Recall: 0.6474\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.65      0.77     45139\n",
      "           1       0.19      0.62      0.29      5931\n",
      "\n",
      "    accuracy                           0.65     51070\n",
      "   macro avg       0.56      0.64      0.53     51070\n",
      "weighted avg       0.84      0.65      0.71     51070\n",
      "\n",
      "\n",
      "Training Gradient Boosting ...\n",
      "Gradient Boosting -> Accuracy: 0.8839, F1: 0.8294, AUC: 0.6472, Precision: 0.7812, Recall: 0.8839\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     45139\n",
      "           1       0.00      0.00      0.00      5931\n",
      "\n",
      "    accuracy                           0.88     51070\n",
      "   macro avg       0.44      0.50      0.47     51070\n",
      "weighted avg       0.78      0.88      0.83     51070\n",
      "\n",
      "\n",
      "Training Naive Bayes ...\n",
      "Naive Bayes -> Accuracy: 0.8839, F1: 0.8295, AUC: 0.6693, Precision: 0.8587, Recall: 0.8839\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94     45139\n",
      "           1       0.67      0.00      0.00      5931\n",
      "\n",
      "    accuracy                           0.88     51070\n",
      "   macro avg       0.78      0.50      0.47     51070\n",
      "weighted avg       0.86      0.88      0.83     51070\n",
      "\n",
      "\n",
      "==== Model Comparison (sorted by AUC) ====\n",
      "                 Model  Accuracy  F1 Score       AUC  Precision    Recall  \\\n",
      "2              XGBoost  0.647366  0.710248  0.687061   0.843675  0.647366   \n",
      "4          Naive Bayes  0.883885  0.829463  0.669262   0.858670  0.883885   \n",
      "0  Logistic Regression  0.616605  0.685462  0.667706   0.838653  0.616605   \n",
      "3    Gradient Boosting  0.883865  0.829378  0.647197   0.781218  0.883865   \n",
      "1        Random Forest  0.883865  0.829378  0.644156   0.781218  0.883865   \n",
      "\n",
      "                                         Best Params  \n",
      "2  {'model__max_depth': 5, 'model__n_estimators':...  \n",
      "4                                                 {}  \n",
      "0  {'model__C': 0.1, 'model__class_weight': 'bala...  \n",
      "3  {'model__learning_rate': 0.05, 'model__n_estim...  \n",
      "1  {'model__max_depth': 3, 'model__n_estimators':...  \n",
      "\n",
      "==== Classification Report: XGBoost ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.65      0.77     45139\n",
      "           1       0.19      0.62      0.29      5931\n",
      "\n",
      "    accuracy                           0.65     51070\n",
      "   macro avg       0.56      0.64      0.53     51070\n",
      "weighted avg       0.84      0.65      0.71     51070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 5. Train, Tune & Compare\n",
    "# ========================\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "#Since dataset is imbalaced and never catching positives of Recall for class 1 = 0.01, so that calculate weights\n",
    "\n",
    "# Calculate class weights: \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "for name, (pipe, params) in models.items():\n",
    "    print(f\"\\nTraining {name} ...\")\n",
    "\n",
    "    # Special handling for XGBoost (scale_pos_weight)\n",
    "    if name == \"XGBoost\":\n",
    "        # ratio of negative/positive class\n",
    "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        if \"model__scale_pos_weight\" not in params:\n",
    "            params[\"model__scale_pos_weight\"] = [scale_pos_weight]\n",
    "        print(f\"Using scale_pos_weight={scale_pos_weight:.2f}\")\n",
    "\n",
    "    # Special handling for Logistic Regression\n",
    "    if name == \"Logistic Regression\":\n",
    "        if \"model__class_weight\" not in params:\n",
    "            params[\"model__class_weight\"] = [\"balanced\"]\n",
    "    \n",
    "    if params:\n",
    "        grid = GridSearchCV(pipe, params, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "    else:\n",
    "        best_model = pipe.fit(X_train, y_train)\n",
    "        best_params = {}\n",
    "    \n",
    "     # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]  # needed for AUC\n",
    "\n",
    "   \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"{name} -> Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Best Params\": best_params\n",
    "    })\n",
    "    best_models[name] = best_model\n",
    "\n",
    "# ========================\n",
    "# 6. Compare Models\n",
    "# ========================\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"AUC\", ascending=False)\n",
    "print(\"\\n==== Model Comparison (sorted by AUC) ====\")\n",
    "print(results_df)\n",
    "\n",
    "# ========================\n",
    "# 7. Classification Report for Best Model\n",
    "# ========================\n",
    "best_row = results_df.iloc[0]\n",
    "best_model_name = best_row[\"Model\"]\n",
    "final_pipeline = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\n==== Classification Report: {best_model_name} ====\")\n",
    "y_pred_best = final_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55cb8c9-6c7f-42b3-958f-40ea951f7ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final pipeline saved as final_pipeline.pkl (XGBoost)\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 8. Save Best Model Pipeline\n",
    "# ========================\n",
    "with open(\"final_pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_pipeline, f)\n",
    "\n",
    "print(f\"\\nFinal pipeline saved as final_pipeline.pkl ({best_model_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d561d7-cfed-46cf-83be-635b1d63a6f8",
   "metadata": {},
   "source": [
    "Handling Class Imbalance\n",
    "\n",
    "In this dataset, the classes are imbalanced: the majority of samples belong to class 0, while class 1 (defaults) are fewer. This imbalance can cause models to predict the majority class more often, leading to high accuracy but poor recall for the minority class.\n",
    "\n",
    "To address this, I evaluated models not only by accuracy, but also by F1-score and AUC (Area Under ROC Curve), which provide a fairer view of performance on imbalanced data.\n",
    "\n",
    "Some models (e.g., Logistic Regression) were trained with the class_weight=\"balanced\" option, which automatically adjusts weights inversely to class frequencies. This helped the model pay more attention to the minority class without modifying the dataset.\n",
    "\n",
    "While oversampling methods such as SMOTE (Synthetic Minority Oversampling Technique) could further improve recall of the minority class, they were not applied here to keep the project simple and focused. This remains an area for future work.\n",
    "\n",
    "Conclusion:\n",
    "The evaluation shows that while accuracy is high across all models, metrics such as F1-score and AUC are more informative for imbalanced datasets. Logistic Regression and XGBoost with class balancing achieved the best balance between metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
